from models.TALNet.TALNet import TALNet
from models.relation_transformer.RelationTransformerModel import RelationTransformerModel, subsequent_mask
import torch.nn as nn
import torch.nn.functional as F
import torch
import pickle
import numpy as np
import math
import warnings
from utils_.config import ConfigDict
from utils_.drop_path import DropPath


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def _init_weights(module: nn.Module, name: str = ''):
    """ ViT weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    """
    if isinstance(module, nn.Linear):
        trunc_normal_(module.weight, std=.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, (nn.Conv2d, nn.Conv1d, nn.Conv3d)):
        # if 'head' in name:
        #     prior = 0.01
        #     module.weight.data.fill_(0)
        #     module.bias.data.fill_(-math.log((1.0 - prior) / prior))
        # else:
        trunc_normal_(module.weight, std=.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(module.bias)
        nn.init.ones_(module.weight)


class VADOR(nn.Module):
    """
    Only BMN confidence calculation has been changed
    """
    def __init__(self, model_cfg):
        super(VADOR, self).__init__()
        with open(model_cfg.relation_transformer.info_path, 'rb') as f:
            infos = pickle.load(f, encoding='latin1')

        self.epoch = -1
        self.usage_frame_features = model_cfg.relation_transformer.frame_features

        self.stage_outputs = 1
        self.output_activation = 'sigmoid'

        opt = infos['opt']
        opt.vocab_size = 0
        opt.encoder_num_layers = model_cfg.relation_transformer.model.encoder.num_layers
        opt.decoder_num_layers = model_cfg.relation_transformer.model.decoder.num_layers
        opt.input_encoding_size = model_cfg.relation_transformer.model.hidden_vector_size
        opt.use_frame_features = self.usage_frame_features
        opt.stage_outputs = self.stage_outputs
        opt.normalize_out = model_cfg.normalize_outlayers
        opt.dropout = model_cfg.relation_transformer.dropout
        opt.drop_path = model_cfg.relation_transformer.drop_path

        # self.eval_kwargs = vars(opt)
        # self.num_frames = model_cfg.relation_transformer.num_frames
        self.feature_generation = model_cfg.feature_generation

        encoder_type = model_cfg.encoder_type if hasattr(model_cfg, 'encoder_type') else 'mix'
        self.relation_transformer = RelationTransformerModel(opt, encoder_type)

        if hasattr(model_cfg.relation_transformer, 'pretrained_weight'):
            checkpoint = torch.load(model_cfg.relation_transformer.pretrained_weight)
            model_dict = self.relation_transformer.state_dict()
            checkpoint = {k: v for k, v in checkpoint.items() if
                          ('encoder' in k or 'att_embed' in k) and k in model_dict}
            model_dict.update(checkpoint)
            self.relation_transformer.load_state_dict(model_dict)

        self.talnet = TALNet(feat_dim=model_cfg.talnet.feat_dim,
                             fpn_levels=model_cfg.talnet.fpn_levels,
                             fpn_repeat=model_cfg.talnet.fpn_repeat)

        self.drop_times = DropPath(drop_prob=model_cfg.talnet.drop_input, scale_by_keep=True)

        self.apply(_init_weights)
        self.talnet.init_head()

        self.model_cfg = model_cfg

    def forward(self, box_features, obj_boxes, clip_features, bmn_mask):
        batch_size, window_size, num_obj, feat_size = box_features.shape[0:4]

        pad_filter = (bmn_mask != 0)[:, :, 0].flatten()

        obj_features = box_features.view(batch_size * window_size, num_obj, feat_size)
        obj_boxes = obj_boxes.view(batch_size * window_size, num_obj, 4)
        clip_features = clip_features.view(batch_size * window_size, 1024, 7, 7)

        obj_features = obj_features[pad_filter]
        obj_boxes = obj_boxes[pad_filter]
        clip_features = clip_features[pad_filter]

        clip_features = self.relation_transformer.clip_feat_block(clip_features)
        obj_features = self.relation_transformer.obj_feat_block(obj_features)

        sequence_len = 1 + num_obj
        action_feat_seqs = []

        obj_feat_seqs = []
        box_groups = []

        cls_vector = self.relation_transformer.cls_embedding_obj(torch.tensor(0).cuda())
        cls_vectors = cls_vector.repeat(len(obj_features), 1, 1)
        box_cls = torch.zeros(1, 1, 4).repeat(len(obj_features), 1, 1).cuda()

        not_objects = torch.zeros((len(obj_features), sequence_len), dtype=torch.bool).cuda()
        not_objects[:, 0] = True
        obj_feat_seqs.append(cls_vectors)
        box_groups.append(box_cls)

        cls_vector = self.relation_transformer.cls_embedding_background(torch.tensor(0).cuda())
        cls_vectors = cls_vector.repeat(len(obj_features), 1, 1)
        action_feat_seqs.append(cls_vectors)

        obj_feat_seqs.append(obj_features)
        box_groups.append(obj_boxes)

        action_feat_seqs.append(clip_features)

        mf_feats = torch.cat(obj_feat_seqs, dim=1)
        mf_boxes = torch.cat(box_groups, dim=1)

        action_feats = torch.cat(action_feat_seqs, dim=1)
        action_feats += self.relation_transformer.position(action_feats)

        mf_enc_feats = self.relation_transformer(mf_feats, mf_boxes,
                                                 action_features=action_feats, not_objects=not_objects,
                                                 mode='sample_feature')

        mf_enc_feats_ = mf_enc_feats[:, 0]

        mf_enc_feats_ = self.drop_times(mf_enc_feats_)
        num_times = (bmn_mask != 0)[:, :, 0].sum(dim=1)

        out_ = []
        start = 0
        for t in num_times:
            if window_size - t > 0:
                padded_window = F.pad(mf_enc_feats_[start:start+t].T, (0, window_size-t))
                # pad_window = self.window_pad(torch.tensor([0] * (window_size - t)).cuda())
                # padded_window = torch.cat([mf_enc_feats_[start:start + t], pad_window]).T
            else:
                padded_window = mf_enc_feats_[start:start+t].T
            out_.append(padded_window)
            start += t

        out_ = torch.cat(out_, dim=1).T
        out_ = out_.reshape(batch_size, window_size, -1)

        out_ = out_.transpose(1, 2)
        confidence_map, box_regression = self.talnet(out_)

        return confidence_map, box_regression
